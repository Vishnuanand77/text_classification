{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Llama2 7b tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb73ff320cd4d268a0270f8447caee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e7f70544c04d909c65c0b174947f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4532e1de144743f2b79bea2f77c3b691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b19b2ff6e34b74a7dec91e8dbd3e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1d00e226e944f393b141cd0e6a9505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ea041e56244eb995cd9ae90180c303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc6ca3851454ba5a8a4797d2e417c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e62ce59dd934292976bd9ec3c40d7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0d4a3b5e3d4f26b17e05613a0e7c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd44df6050dc4883b8ea4807be4e730c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09466a3a6244c3c9d9ba3148ee73cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a63b98aefc9418190d0600de662a625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Define the model name and cache directory\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "cache_dir = \"/scratch/gilbreth/anand173/model_cache\"\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Use 4-bit quantization\n",
    "    bnb_4bit_use_double_quant=True,  # Enable double quantization for memory savings\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Use bfloat16 for computation\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "\n",
    "# Load the model with 4-bit quantization and device map\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Automatically allocate model layers across GPU/CPU\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "# Ensure the pad token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero shot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating classification response...\n",
      "Predicted Label: ### Instruction:\n",
      "Classify the following review into \"Correct Size/Just Right\", \"Wrong Size\", \"No Comment\". Please respond only with the category:\n",
      "\n",
      "### Input:\n",
      "Item was delivered on time and was a direct replcement\n",
      "\n",
      "### Response: \n",
      "Correct Size/Just Right\n",
      "\n",
      "### Instruction:\n",
      "Runtime: 0.86 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Example review to classify\n",
    "review = \"Item was delivered on time and was a direct replcement\"\n",
    "\n",
    "# Format prompt for classification\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Classify the following review into \"Correct Size/Just Right\", \"Wrong Size\", \"No Comment\". Please respond only with the category:\n",
    "\n",
    "### Input:\n",
    "{review}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    ").to(\"cuda\")  # Send input tensors to GPU\n",
    "\n",
    "# Generate the output\n",
    "print(\"Generating classification response...\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=10,           # Limit the response length\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and display the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "print(f\"Predicted Label: {response}\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Print runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Runtime: {runtime:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating classification response...\n",
      "Predicted Label: ### Instruction:\n",
      "Classify the following autoparts review into \"Correct Size/Just Right\", \"Wrong Size\", \"No Comment\". Please respond only with the category:\n",
      "\n",
      "\n",
      "### Examples:\n",
      "1. Review: \"order came quickly and is working fine and is much better price than going to Lowe's or Home Depot to purchase.\"\n",
      "   Classification -> No Comment\n",
      "2. Review: \"Perfect Fit - Ideal for when you don't need to replace an otherwise good OEM axle. Fits all FWD/AWD Volvo 850   S/V70 '93-'00\"\n",
      "   Classification -> Correct Size/Just Right\n",
      "3. Review: \"two different ends on cables. doesn't make sense. had to change the end on one side to fit it to the battery.\"\n",
      "   Classification -> Wrong Size\n",
      "\n",
      "\n",
      "### Input:\n",
      "Review: \"Have not had to use it yet - but I know how handy it is to have it available. Thanks\"\n",
      "### Response:\n",
      "No Comment\n",
      "\n",
      "### Input:\n",
      "Review: \"fits\n",
      "Runtime: 0.84 seconds\n"
     ]
    }
   ],
   "source": [
    "# Few-shot examples for the classification task\n",
    "few_shot_examples = \"\"\"\n",
    "### Examples:\n",
    "1. Review: \"order came quickly and is working fine and is much better price than going to Lowe's or Home Depot to purchase.\"\n",
    "   Classification -> No Comment\n",
    "2. Review: \"Perfect Fit - Ideal for when you don't need to replace an otherwise good OEM axle. Fits all FWD/AWD Volvo 850   S/V70 '93-'00\"\n",
    "   Classification -> Correct Size/Just Right\n",
    "3. Review: \"two different ends on cables. doesn't make sense. had to change the end on one side to fit it to the battery.\"\n",
    "   Classification -> Wrong Size\n",
    "\"\"\"\n",
    "\n",
    "# Example review to classify\n",
    "review = \"Have not had to use it yet - but I know how handy it is to have it available. Thanks\"\n",
    "\n",
    "# Format prompt with few-shot examples\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Classify the following autoparts review into \"Correct Size/Just Right\", \"Wrong Size\", \"No Comment\". Please respond only with the category:\n",
    "\n",
    "{few_shot_examples}\n",
    "\n",
    "### Input:\n",
    "Review: \"{review}\"\n",
    "### Response:\n",
    "\"\"\"\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    ").to(\"cuda\")  # Send input tensors to GPU\n",
    "\n",
    "# Generate the output\n",
    "print(\"Generating classification response...\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=10,           # Limit the response length\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and display the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "print(f\"Predicted Label: {response}\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Print runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Runtime: {runtime:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First 10 reviews prompt version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: You will have to remove the window which is very easy and be very careful.  drop the window down to allow access to both screws holding the glass.  Lift the window up a bit and then drop the front of the windows down into the door slowly and then raise the rear of the glass up and you will start to lift the glass at a angle upwards and out of the door frame.  You will then be able to access the three screws to remove the door handle and you will only have to remove the cable pin from the door once you have access to the back of the handle.The process to remove the door handles is very very easy and you will need a P2 and P3 screw driver and a 10mm socket with extension.\n",
      "Predicted Label: Correct Size/Just Right. ### Review:\n",
      "I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:  It does what it is supposed to! Sure it does not come with any instructions, why take a star away for that? Once I got the power connected to it the correct way, the unit kicks on and off solidly. There is some audible  #34;clicking #34; when the relay switches on and off, so don't worry.The BEST thing about the unit is the sensitivity adjustment. I hooked one of these up to an LED strip light going down my staircase. Lights go out, stair lights come on. There are lights in the hallway at the top of the stairs, chandelier over the staircase and natural sunlight from the side transitions on the front door. Once everything was connected I was able to adjust the sensitivity to all the lighting. Perfect!Follow the instructions from Luke and Aaron in these reviews. That should be all that you need.Could this product be better? Maybe, but for the price and how solidly it is built, I doubt you could do better. Definitely use it indoor only. Or in a water tight box if outside. Not sure how it would stand up to freezing, etc.As long as the photo eye holds out for a while, I'm satisfied.I have purchased another and will hook it up to another LED strip light configuration for over and under cabinet lighting. \n",
      "Predicted Label: \"Correct Size/Just Right\"  ### Review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: Item was delivered on time and was a direct replcement\n",
      "Predicted Label: Correct Size/Just Right ### Review:\n",
      "This shirt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:  This was a really great part, shipped fast, and was as decribed.  Looked great once installed with my single feed fuel line! \n",
      "Predicted Label: Correct Size/Just Right\n",
      "\n",
      "### Review:\n",
      "I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This puller worked getting off a stubborn wiper arm but after one use a piece broke off. It was still worth what was paid.\n",
      "Predicted Label: Correct Size/Just Right ### Review:\n",
      "I was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:  Works ok, really just a quick adapter as the comfort lights go fom green to amber nearly immediatly \n",
      "Predicted Label: No Comment. ### Review:\n",
      "It is a bit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: Shipped really fast.  I've had it about a month now and it is working perfectly.  Pay attention to what size your connection requirement is (width) - .187 or .250 inch.  This is 1/4 inch.\n",
      "Predicted Label: Correct Size/Just Right ### Review:\n",
      "The item\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:  I don't love this, it's just a battery.  But it is exactly as advertised, competitively priced, and appropriate for its intended use. \n",
      "Predicted Label: Correct Size/Just Right\n",
      "\n",
      "### Review:\n",
      "I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:  It only last 1 year and i couldn't find where to get the free 18 months replacement that appears on the warranty. I sent emails to Exide Batteries, and have no answers... it is incredible what a scam these batteries are and there is no warranty, nobody will replace your battery or your money back. Yesterday, the battery on my car died, it was another brand from walmart it last 2 years and 10 months, i just went to the store WITHOUT receipt of purchase,and still got a new one for 2.06$ !!!! and still with 3 years of free replacement!! \n",
      "Predicted Label: \"Wrong Size\"  ### Review:\n",
      "I am\n",
      "Review:  The vendor immediately phoned me, listened carefully to the problem, understood the problem (which is very rare today), and promptly mailed the correct fixtures to adapt the 3/8 #34; female lugs that are wired to my Conext Model Up 300 to the 1/4 #34; male lugs which are mounted to the battery.  I could not be happier.  I was made to feel super important.  And unlike practically all other vendors today - there was no attempt to blame it on me.  The response was  #34;all ears #34;, contained very little gratuitous back-chat, and resulted in immediate action.  My day was improved immensely by the way this vendor handled what was actually a very small problem that we could have let slide.  But the simple fact of the matter is that firm, clean, and snug electrical connections are at the heart of safe electronics.  It was indeed possible to make the wiring absolutely correct.  And the vendor made it so.  Yay for 1-800-battery! \n",
      "Predicted Label: \"Correct Size/Just Right\" ### Instruction:\n",
      "Predictions for the first 10 reviews saved to fit_predictions_first_10.csv.\n",
      "Runtime: 8.00 seconds\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Define the file path for the input reviews\n",
    "input_file = \"fit.csv\"\n",
    "output_file = \"fit_predictions_first_10.csv\"\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Prepare to write results to a new CSV file\n",
    "with open(output_file, mode=\"w\", newline=\"\") as out_csv:\n",
    "    writer = csv.writer(out_csv)\n",
    "    writer.writerow([\"ReviewText\", \"PredictedLabel\"])  # Write headers\n",
    "\n",
    "    # Initialize a counter\n",
    "    review_count = 0\n",
    "\n",
    "    # Read and process each review from the input CSV file\n",
    "    with open(input_file, mode=\"r\") as in_csv:\n",
    "        reader = csv.DictReader(in_csv)\n",
    "        for row in reader:\n",
    "            if review_count >= 10:  # Process only the first 10 reviews\n",
    "                break\n",
    "\n",
    "            review = row[\"ReviewText\"]\n",
    "\n",
    "            # Format the prompt for each review\n",
    "            prompt = f\"\"\"### Instruction:\n",
    "You are an assistant tasked with classifying reviews into one of the categories: \"Correct Size/Just Right\", \"Wrong Size\", or \"No Comment\".\n",
    "Respond **only** with the category name: \"Correct Size/Just Right\", \"Wrong Size\", or \"No Comment\". Do not include any other text or explanation.\n",
    "\n",
    "### Categories:\n",
    "1. Correct Size/Just Right: The product fits as expected and performs its intended function without issues.\n",
    "2. Wrong Size: The product does not fit or requires modifications to work correctly.\n",
    "3. No Comment: The review does not mention size or fitting issues.\n",
    "\n",
    "### Review:\n",
    "{review}\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "            # Tokenize the input\n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=512\n",
    "            ).to(\"cuda\")  # Send input tensors to GPU\n",
    "\n",
    "            # Generate the output\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,  # Limit the response length\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            # Decode the response and clean it\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "            # Extract the category after \"### Response:\"\n",
    "            if \"### Response:\" in response:\n",
    "                category = response.split(\"### Response:\")[-1].strip()\n",
    "            else:\n",
    "                category = \"Invalid Response\"  # Fallback if the format is incorrect\n",
    "\n",
    "            print(f\"Review: {review}\")\n",
    "            print(f\"Predicted Label: {category}\")\n",
    "\n",
    "            # Write the review and predicted label to the output CSV\n",
    "            writer.writerow([review, category])\n",
    "\n",
    "            # Increment the counter\n",
    "            review_count += 1\n",
    "\n",
    "print(f\"Predictions for the first 10 reviews saved to {output_file}.\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Print runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Runtime: {runtime:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Predicted vs. Actual:\n",
      "                                          ReviewText                FINAL Fit  \\\n",
      "0  You will have to remove the window which is ve...               No Comment   \n",
      "1   It does what it is supposed to! Sure it does ...               No Comment   \n",
      "2  Item was delivered on time and was a direct re...               No Comment   \n",
      "3   This was a really great part, shipped fast, a...               No Comment   \n",
      "4  This puller worked getting off a stubborn wipe...               No Comment   \n",
      "5   Works ok, really just a quick adapter as the ...               No Comment   \n",
      "6  Shipped really fast.  I've had it about a mont...  Correct Size/Just Right   \n",
      "7   I don't love this, it's just a battery.  But ...               No Comment   \n",
      "8   It only last 1 year and i couldn't find where...               No Comment   \n",
      "9   The vendor immediately phoned me, listened ca...  Correct Size/Just Right   \n",
      "\n",
      "            PredictedLabel  Match  \n",
      "0  Correct Size/Just Right  False  \n",
      "1  Correct Size/Just Right  False  \n",
      "2  Correct Size/Just Right  False  \n",
      "3  Correct Size/Just Right  False  \n",
      "4  Correct Size/Just Right  False  \n",
      "5  Correct Size/Just Right  False  \n",
      "6  Correct Size/Just Right   True  \n",
      "7  Correct Size/Just Right  False  \n",
      "8  Correct Size/Just Right  False  \n",
      "9  Correct Size/Just Right   True  \n",
      "\n",
      "Accuracy: 20.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "input_file = \"fit.csv\"  # Original file with actual labels\n",
    "predictions_file = \"fit_predictions_first_10.csv\"  # File with model predictions\n",
    "\n",
    "# Load input and prediction files as DataFrames\n",
    "df_input = pd.read_csv(input_file)\n",
    "df_predictions = pd.read_csv(predictions_file)\n",
    "\n",
    "# Ensure only the first 10 rows are used for comparison\n",
    "df_input = df_input.head(10)\n",
    "\n",
    "# Combine DataFrames for comparison\n",
    "# Use \"ReviewText\" as the matching key\n",
    "comparison_df = pd.merge(\n",
    "    df_input, \n",
    "    df_predictions, \n",
    "    on=\"ReviewText\", \n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Compare the 'FINAL Fit' column with 'PredictedLabel'\n",
    "comparison_df[\"Match\"] = comparison_df[\"FINAL Fit\"] == comparison_df[\"PredictedLabel\"]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = comparison_df[\"Match\"].mean()\n",
    "\n",
    "# Display results\n",
    "print(\"Comparison of Predicted vs. Actual:\")\n",
    "print(comparison_df[[\"ReviewText\", \"FINAL Fit\", \"PredictedLabel\", \"Match\"]])\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First 10 reviews prompt v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for the first 10 reviews saved to fit_predictions_first_10_promptv2.csv.\n",
      "Runtime: 7.60 seconds\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Define the file path for the input reviews\n",
    "input_file = \"fit.csv\"\n",
    "output_file = \"fit_predictions_first_10_promptv2.csv\"\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Prepare to write results to a new CSV file\n",
    "with open(output_file, mode=\"w\", newline=\"\") as out_csv:\n",
    "    writer = csv.writer(out_csv)\n",
    "    writer.writerow([\"ReviewText\", \"PredictedLabel\"])  # Write headers\n",
    "\n",
    "    # Initialize a counter\n",
    "    review_count = 0\n",
    "\n",
    "    # Read and process each review from the input CSV file\n",
    "    with open(input_file, mode=\"r\") as in_csv:\n",
    "        reader = csv.DictReader(in_csv)\n",
    "        for row in reader:\n",
    "            if review_count >= 10:  # Process only the first 10 reviews\n",
    "                break\n",
    "\n",
    "            review = row[\"ReviewText\"]\n",
    "\n",
    "            # Format the prompt for each review\n",
    "            prompt = f\"\"\"nstruction: Classify the following review into one of the categories: \"Correct Size/Just Right,\" \"Wrong Size,\" or \"No Comment.\"\n",
    "Examples:\n",
    "1. \"Fits perfectly and works well with my setup.\" -> Correct Size/Just Right\n",
    "2. \"I had to modify it to make it fit my device.\" -> Wrong Size\n",
    "3. \"Shipped on time and is good quality.\" -> No Comment\n",
    "Review: {review}\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "            # Tokenize the input\n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=512\n",
    "            ).to(\"cuda\")  # Send input tensors to GPU\n",
    "\n",
    "            # Generate the output\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,  # Limit the response length\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            # Decode the response and clean it\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "            # Extract only the classification label cleanly\n",
    "            if \"Classification ->\" in response:\n",
    "                response = response.split(\"Classification ->\")[-1].split(\"\\n\")[0].strip()\n",
    "            else:\n",
    "                # If no proper format, default to \"No Comment\" for robustness\n",
    "                response = \"No Comment\"\n",
    "\n",
    "            # Write the review and predicted label to the output CSV\n",
    "            writer.writerow([review, response])\n",
    "\n",
    "            # Increment the counter\n",
    "            review_count += 1\n",
    "\n",
    "print(f\"Predictions for the first 10 reviews saved to {output_file}.\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Print runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Runtime: {runtime:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Predicted vs. Actual:\n",
      "                                          ReviewText                FINAL Fit  \\\n",
      "0  You will have to remove the window which is ve...               No Comment   \n",
      "1   It does what it is supposed to! Sure it does ...               No Comment   \n",
      "2  Item was delivered on time and was a direct re...               No Comment   \n",
      "3   This was a really great part, shipped fast, a...               No Comment   \n",
      "4  This puller worked getting off a stubborn wipe...               No Comment   \n",
      "5   Works ok, really just a quick adapter as the ...               No Comment   \n",
      "6  Shipped really fast.  I've had it about a mont...  Correct Size/Just Right   \n",
      "7   I don't love this, it's just a battery.  But ...               No Comment   \n",
      "8   It only last 1 year and i couldn't find where...               No Comment   \n",
      "9   The vendor immediately phoned me, listened ca...  Correct Size/Just Right   \n",
      "\n",
      "  PredictedLabel  Match  \n",
      "0     No Comment   True  \n",
      "1     No Comment   True  \n",
      "2     No Comment   True  \n",
      "3     No Comment   True  \n",
      "4     No Comment   True  \n",
      "5     No Comment   True  \n",
      "6     No Comment  False  \n",
      "7     No Comment   True  \n",
      "8     No Comment   True  \n",
      "9     No Comment  False  \n",
      "\n",
      "Accuracy: 80.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "input_file = \"fit.csv\"  # Original file with actual labels\n",
    "predictions_file = \"fit_predictions_first_10_promptv2.csv\"  # File with model predictions\n",
    "\n",
    "# Load input and prediction files as DataFrames\n",
    "df_input = pd.read_csv(input_file)\n",
    "df_predictions = pd.read_csv(predictions_file)\n",
    "\n",
    "# Ensure only the first 10 rows are used for comparison\n",
    "df_input = df_input.head(10)\n",
    "\n",
    "# Combine DataFrames for comparison\n",
    "# Use \"ReviewText\" as the matching key\n",
    "comparison_df = pd.merge(\n",
    "    df_input, \n",
    "    df_predictions, \n",
    "    on=\"ReviewText\", \n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Compare the 'FINAL Fit' column with 'PredictedLabel'\n",
    "comparison_df[\"Match\"] = comparison_df[\"FINAL Fit\"] == comparison_df[\"PredictedLabel\"]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = comparison_df[\"Match\"].mean()\n",
    "\n",
    "# Display results\n",
    "print(\"Comparison of Predicted vs. Actual:\")\n",
    "print(comparison_df[[\"ReviewText\", \"FINAL Fit\", \"PredictedLabel\", \"Match\"]])\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First 50 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 46\u001b[0m\n\u001b[1;32m     37\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m     38\u001b[0m     prompt,\n\u001b[1;32m     39\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m\n\u001b[1;32m     43\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Send input tensors to GPU\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Generate the output\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[1;32m     48\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,  \u001b[38;5;66;03m# Limit the response length\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     eos_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Decode the response and clean it\u001b[39;00m\n\u001b[1;32m     53\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/transformers/generation/utils.py:2252\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2245\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2246\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2247\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2249\u001b[0m     )\n\u001b[1;32m   2251\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2252\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[1;32m   2253\u001b[0m         input_ids,\n\u001b[1;32m   2254\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   2255\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   2256\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   2257\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   2258\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   2259\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2260\u001b[0m     )\n\u001b[1;32m   2262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2265\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2266\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2272\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/transformers/generation/utils.py:3254\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3252\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3254\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3257\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3258\u001b[0m     outputs,\n\u001b[1;32m   3259\u001b[0m     model_kwargs,\n\u001b[1;32m   3260\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3261\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1163\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1160\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1163\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1164\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1165\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1166\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1167\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1168\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1169\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1170\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1171\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1172\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1173\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1174\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1175\u001b[0m )\n\u001b[1;32m   1177\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:913\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    902\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    903\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         position_embeddings,\n\u001b[1;32m    911\u001b[0m     )\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 913\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    914\u001b[0m         hidden_states,\n\u001b[1;32m    915\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m    916\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    917\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    918\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    919\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    920\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    921\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    922\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    925\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:655\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    654\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 655\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    656\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    657\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:73\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     71\u001b[0m input_dtype \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m     72\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 73\u001b[0m variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     74\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Define the file path for the input reviews\n",
    "input_file = \"fit.csv\"\n",
    "output_file = \"fit_predictions_first_50.csv\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Prepare to write results to a new CSV file\n",
    "with open(output_file, mode=\"w\", newline=\"\") as out_csv:\n",
    "    writer = csv.writer(out_csv)\n",
    "    writer.writerow([\"ReviewText\", \"PredictedLabel\"])  # Write headers\n",
    "\n",
    "    # Initialize a counter\n",
    "    review_count = 0\n",
    "\n",
    "    # Read and process each review from the input CSV file\n",
    "    with open(input_file, mode=\"r\") as in_csv:\n",
    "        reader = csv.DictReader(in_csv)\n",
    "        for row in reader:\n",
    "            if review_count >= 50:  # Process only the first 10 reviews\n",
    "                break\n",
    "\n",
    "            review = row[\"ReviewText\"]\n",
    "\n",
    "            # Format the prompt for each review\n",
    "            prompt = f\"\"\"Instruction: Classify the following review into one of the categories: \"Correct Size/Just Right,\" \"Wrong Size,\" or \"No Comment.\". Do not include anything other than the category itself.\n",
    "Examples:\n",
    "1. \"Fits perfectly and works well with my setup.\" -> Correct Size/Just Right\n",
    "2. \"I had to modify it to make it fit my device.\" -> Wrong Size\n",
    "3. \"Shipped on time and is good quality.\" -> No Comment\n",
    "Review: {review}\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "            # Tokenize the input\n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=512\n",
    "            ).to(\"cuda\")  # Send input tensors to GPU\n",
    "\n",
    "            # Generate the output\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,  # Limit the response length\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            # Decode the response and clean it\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "            # Extract only the classification label cleanly\n",
    "            if \"Classification ->\" in response:\n",
    "                response = response.split(\"Classification ->\")[-1].split(\"\\n\")[0].strip()\n",
    "            else:\n",
    "                # If no proper format, default to \"No Comment\" for robustness\n",
    "                response = \"No Comment\"\n",
    "\n",
    "            # Write the review and predicted label to the output CSV\n",
    "            writer.writerow([review, response])\n",
    "\n",
    "            # Increment the counter\n",
    "            review_count += 1\n",
    "\n",
    "print(f\"Predictions for the first 50 reviews saved to {output_file}.\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Print runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Runtime: {runtime:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Predicted vs. Actual:\n",
      "                                           ReviewText  \\\n",
      "0   You will have to remove the window which is ve...   \n",
      "1    It does what it is supposed to! Sure it does ...   \n",
      "2   Item was delivered on time and was a direct re...   \n",
      "3    This was a really great part, shipped fast, a...   \n",
      "4   This puller worked getting off a stubborn wipe...   \n",
      "5    Works ok, really just a quick adapter as the ...   \n",
      "6   Shipped really fast.  I've had it about a mont...   \n",
      "7    I don't love this, it's just a battery.  But ...   \n",
      "8    It only last 1 year and i couldn't find where...   \n",
      "9    The vendor immediately phoned me, listened ca...   \n",
      "10  Battery fired up on the first try and couldn't...   \n",
      "11  Everyone else was sold out... our kids love ri...   \n",
      "12   If quality matters to you, and you are lookin...   \n",
      "13   This battery does not have F2 terminals. They...   \n",
      "14  Looked far and wide for this battery as a repl...   \n",
      "15   Husband bought this for his motorcycle, and l...   \n",
      "16   I was a bit worried about ordering a battery ...   \n",
      "17  Quick shipping. So far the battery is living u...   \n",
      "18  Thanks for a great product. The batteries were...   \n",
      "19  I put this into a 1999 Mazda Miata MX-5.  It f...   \n",
      "20  This is an excellent product and works as adve...   \n",
      "21   I have not yet had the opportunity to use thi...   \n",
      "22   Product was delivered on time, and works well...   \n",
      "23  Worked like it was advertised and lots cheaper...   \n",
      "24  This seem to satify my needs and it was as che...   \n",
      "25   My low battery alarm started going off in the...   \n",
      "26  I was very pleased with the battery shipping w...   \n",
      "27  works as advertised except for the narrower te...   \n",
      "28  Battery only lasted 1 month after a cold spell...   \n",
      "29  Our CF Moto trike was purchased in 11/12   we ...   \n",
      "30   Even though we charged the battery for well o...   \n",
      "31   Nothing special about this one.  This item wa...   \n",
      "32  Its the battery i was looking to buy. It charg...   \n",
      "33   I liked the price and the previous reviews I ...   \n",
      "34   The acid delivery / installation system is we...   \n",
      "35  battery only lasted around 4-5 months then sud...   \n",
      "36  This battery was exactly what we needed. the c...   \n",
      "37   Installed battery right out of the shipping b...   \n",
      "38   Really good battery for the price, very usefu...   \n",
      "39  So far so good with this battery. We got this ...   \n",
      "40   Cheaper to order and install yourself instead...   \n",
      "41  The battery didn't hold a charge for more than...   \n",
      "42  Everything I wanted and more. This is a must h...   \n",
      "43   I ordered this item so I could do a little of...   \n",
      "44   I used this battery in my Fire Burglar Instru...   \n",
      "45  This is a good battery.  The company is respon...   \n",
      "46   This is a great company. Got two for my sonsE...   \n",
      "47  I have bought 2 of these batteries from this s...   \n",
      "48  I just installed it today in my FIOS system.  ...   \n",
      "49   The battery came verry quick, looked and fit ...   \n",
      "\n",
      "                  FINAL Fit PredictedLabel  Match  \n",
      "0                No Comment     No Comment   True  \n",
      "1                No Comment     No Comment   True  \n",
      "2                No Comment     No Comment   True  \n",
      "3                No Comment     No Comment   True  \n",
      "4                No Comment     No Comment   True  \n",
      "5                No Comment     No Comment   True  \n",
      "6   Correct Size/Just Right     No Comment  False  \n",
      "7                No Comment     No Comment   True  \n",
      "8                No Comment     No Comment   True  \n",
      "9   Correct Size/Just Right     No Comment  False  \n",
      "10               No Comment     No Comment   True  \n",
      "11               No Comment     No Comment   True  \n",
      "12               No Comment     No Comment   True  \n",
      "13  Correct Size/Just Right     No Comment  False  \n",
      "14  Correct Size/Just Right     No Comment  False  \n",
      "15               No Comment     No Comment   True  \n",
      "16               No Comment     No Comment   True  \n",
      "17               No Comment     No Comment   True  \n",
      "18               No Comment     No Comment   True  \n",
      "19  Correct Size/Just Right     No Comment  False  \n",
      "20               No Comment     No Comment   True  \n",
      "21               No Comment     No Comment   True  \n",
      "22  Correct Size/Just Right     No Comment  False  \n",
      "23               No Comment     No Comment   True  \n",
      "24               No Comment     No Comment   True  \n",
      "25               No Comment     No Comment   True  \n",
      "26               No Comment     No Comment   True  \n",
      "27  Correct Size/Just Right     No Comment  False  \n",
      "28               No Comment     No Comment   True  \n",
      "29               No Comment     No Comment   True  \n",
      "30               No Comment     No Comment   True  \n",
      "31  Correct Size/Just Right     No Comment  False  \n",
      "32               No Comment     No Comment   True  \n",
      "33               No Comment     No Comment   True  \n",
      "34  Correct Size/Just Right     No Comment  False  \n",
      "35               No Comment     No Comment   True  \n",
      "36               No Comment     No Comment   True  \n",
      "37               No Comment     No Comment   True  \n",
      "38               No Comment     No Comment   True  \n",
      "39               No Comment     No Comment   True  \n",
      "40  Correct Size/Just Right     No Comment  False  \n",
      "41               No Comment     No Comment   True  \n",
      "42               No Comment     No Comment   True  \n",
      "43               No Comment     No Comment   True  \n",
      "44               No Comment     No Comment   True  \n",
      "45               No Comment     No Comment   True  \n",
      "46               No Comment     No Comment   True  \n",
      "47               No Comment     No Comment   True  \n",
      "48               No Comment     No Comment   True  \n",
      "49  Correct Size/Just Right     No Comment  False  \n",
      "\n",
      "Accuracy: 78.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "input_file = \"fit.csv\"  # Original file with actual labels\n",
    "predictions_file = \"fit_predictions_first_50.csv\"  # File with model predictions\n",
    "\n",
    "# Load input and prediction files as DataFrames\n",
    "df_input = pd.read_csv(input_file)\n",
    "df_predictions = pd.read_csv(predictions_file)\n",
    "\n",
    "# Ensure only the first 10 rows are used for comparison\n",
    "df_input = df_input.head(50)\n",
    "\n",
    "# Combine DataFrames for comparison\n",
    "# Use \"ReviewText\" as the matching key\n",
    "comparison_df = pd.merge(\n",
    "    df_input, \n",
    "    df_predictions, \n",
    "    on=\"ReviewText\", \n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Compare the 'FINAL Fit' column with 'PredictedLabel'\n",
    "comparison_df[\"Match\"] = comparison_df[\"FINAL Fit\"] == comparison_df[\"PredictedLabel\"]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = comparison_df[\"Match\"].mean()\n",
    "\n",
    "# Display results\n",
    "print(\"Comparison of Predicted vs. Actual:\")\n",
    "print(comparison_df[[\"ReviewText\", \"FINAL Fit\", \"PredictedLabel\", \"Match\"]])\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot first 500 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews: 2255\n",
      "Reviews that would be truncated: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load dataset\n",
    "input_file = \"fit.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = \"\"\"### Instruction:\n",
    "    Classify the following review into one of the categories: \"Correct Size/Just Right,\" \"Wrong Size,\" or \"No Comment.\"\n",
    "    Respond only with the category name.\n",
    "\n",
    "    ### Categories:\n",
    "    1. Correct Size/Just Right: The product fits as expected and performs its intended function without issues.\n",
    "    2. Wrong Size: The product does not fit or requires modifications to work correctly.\n",
    "    3. No Comment: The review does not mention size or fitting issues.\n",
    "\n",
    "    ### Examples:\n",
    "    1. \"I put this into a 1999 Mazda Miata MX-5.  It fit perfectly.  It came fully charged.  Delivered it saved me $50.  Great deal.  Love it.\" -> Correct Size/Just Right\n",
    "    2. \"I have even used this to start my dodge 2500 which has a heavy duty battery for starting and it worked great. The light pulls out to expose a 12v car adapter socket.\" -> Correct Size/Just Right\n",
    "    3. \"It was not the exact match. I had to rewire the battery in order to make it work. It was a toy for my Lil man. I am glad that I was able to to make it work.  But make sure you can iuse it.\" -> Wrong Size\n",
    "    4. \"two different ends on cables. doesn't make sense. had to change the end on one side to fit it to the battery.\" -> Wrong Size\n",
    "    5. \"I would recommend this product.  It lasts long and works fine.  Did the job for me.  It was a good price.\" -> No Comment\n",
    "\n",
    "    ### Review:\n",
    "    {review}\n",
    "    ### Response:\n",
    "\"\"\"\n",
    "\n",
    "# Define max length\n",
    "max_length = 1500\n",
    "\n",
    "# Check token lengths\n",
    "truncated_count = 0\n",
    "for review in df[\"ReviewText\"]:\n",
    "    # Create the full prompt for the review\n",
    "    prompt = prompt_template.format(review=review)\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    tokens = tokenizer(prompt, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "    token_count = tokens[\"input_ids\"].shape[-1]\n",
    "    \n",
    "    # Check if the token count exceeds the max_length\n",
    "    if token_count > max_length:\n",
    "        truncated_count += 1\n",
    "\n",
    "# Output the result\n",
    "print(f\"Total reviews: {len(df)}\")\n",
    "print(f\"Reviews that would be truncated: {truncated_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for the first 500 reviews saved to fit_predictions_first_500.csv.\n",
      "Runtime: 643.10 seconds\n",
      "Comparison of Predicted vs. Actual:\n",
      "                                            ReviewText  \\\n",
      "0    You will have to remove the window which is ve...   \n",
      "1     It does what it is supposed to! Sure it does ...   \n",
      "2    Item was delivered on time and was a direct re...   \n",
      "3     This was a really great part, shipped fast, a...   \n",
      "4    This puller worked getting off a stubborn wipe...   \n",
      "..                                                 ...   \n",
      "495   If you have the rear defrost then this is not...   \n",
      "496   It's just a bit noisier than the factory Bosc...   \n",
      "497  This is the 3rd radiator my husband has ordere...   \n",
      "498   This was very easy to install and a necessary...   \n",
      "499   This liquid epoxy product repaired the crack ...   \n",
      "\n",
      "                   FINAL Fit PredictedLabel  Match  \n",
      "0                 No Comment     No Comment   True  \n",
      "1                 No Comment     No Comment   True  \n",
      "2                 No Comment     No Comment   True  \n",
      "3                 No Comment     No Comment   True  \n",
      "4                 No Comment     No Comment   True  \n",
      "..                       ...            ...    ...  \n",
      "495  Correct Size/Just Right     No Comment  False  \n",
      "496               No Comment     No Comment   True  \n",
      "497               No Comment     No Comment   True  \n",
      "498               No Comment     No Comment   True  \n",
      "499               No Comment     No Comment   True  \n",
      "\n",
      "[500 rows x 4 columns]\n",
      "\n",
      "Accuracy: 63.40%\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Define the file path for the input reviews\n",
    "input_file = \"fit.csv\"\n",
    "output_file = \"fit_predictions_first_500.csv\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Prepare to write results to a new CSV file\n",
    "with open(output_file, mode=\"w\", newline=\"\") as out_csv:\n",
    "    writer = csv.writer(out_csv)\n",
    "    writer.writerow([\"ReviewText\", \"PredictedLabel\"])  # Write headers\n",
    "\n",
    "    # Initialize a counter\n",
    "    review_count = 0\n",
    "\n",
    "    # Read and process each review from the input CSV file\n",
    "    with open(input_file, mode=\"r\") as in_csv:\n",
    "        reader = csv.DictReader(in_csv)\n",
    "        for row in reader:\n",
    "            if review_count >= 500:  # Process only the first 10 reviews\n",
    "                break\n",
    "\n",
    "            review = row[\"ReviewText\"]\n",
    "\n",
    "            prompt = f\"\"\"### Instruction:\n",
    "    Classify the following review into one of the categories: \"Correct Size/Just Right,\" \"Wrong Size,\" or \"No Comment.\"\n",
    "    Respond only with the category name.\n",
    "\n",
    "    ### Categories:\n",
    "    1. Correct Size/Just Right: The product fits as expected and performs its intended function without issues.\n",
    "    2. Wrong Size: The product does not fit or requires modifications to work correctly.\n",
    "    3. No Comment: The review does not mention size or fitting issues.\n",
    "\n",
    "    ### Examples:\n",
    "    1. \"I put this into a 1999 Mazda Miata MX-5.  It fit perfectly.  It came fully charged.  Delivered it saved me $50.  Great deal.  Love it.\" -> Correct Size/Just Right\n",
    "    2. \"It was not the exact match. I had to rewire the battery in order to make it work. It was a toy for my Lil man. I am glad that I was able to to make it work.  But make sure you can iuse it.\" -> Wrong Size\n",
    "    3. \"I would recommend this product.  It lasts long and works fine.  Did the job for me.  It was a good price.\" -> No Comment\n",
    "\n",
    "    ### Review:\n",
    "    {review}\n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "\n",
    "            # Tokenize the input\n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=1024\n",
    "            ).to(\"cuda\")  # Send input tensors to GPU\n",
    "\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=20,  # Adjust for a longer response window\n",
    "                temperature=0.7,    # Adds randomness; lower values make output deterministic\n",
    "                top_p=0.9,          # Nucleus sampling\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            # Decode the response and clean it\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "            # Extract only the classification label cleanly\n",
    "            if \"Classification ->\" in response:\n",
    "                response = response.split(\"Classification ->\")[-1].split(\"\\n\")[0].strip()\n",
    "            else:\n",
    "                # If no proper format, default to \"No Comment\" for robustness\n",
    "                response = \"No Comment\"\n",
    "\n",
    "            # Write the review and predicted label to the output CSV\n",
    "            writer.writerow([review, response])\n",
    "\n",
    "            # Increment the counter\n",
    "            review_count += 1\n",
    "\n",
    "print(f\"Predictions for the first 500 reviews saved to {output_file}.\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Print runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Runtime: {runtime:.2f} seconds\")\n",
    "\n",
    "# File paths\n",
    "input_file = \"fit.csv\"  # Original file with actual labels\n",
    "predictions_file = \"fit_predictions_first_500.csv\"  # File with model predictions\n",
    "\n",
    "# Load input and prediction files as DataFrames\n",
    "df_input = pd.read_csv(input_file)\n",
    "df_predictions = pd.read_csv(predictions_file)\n",
    "\n",
    "df_input = df_input.head(500)\n",
    "\n",
    "# Combine DataFrames for comparison\n",
    "# Use \"ReviewText\" as the matching key\n",
    "comparison_df = pd.merge(\n",
    "    df_input, \n",
    "    df_predictions, \n",
    "    on=\"ReviewText\", \n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Compare the 'FINAL Fit' column with 'PredictedLabel'\n",
    "comparison_df[\"Match\"] = comparison_df[\"FINAL Fit\"] == comparison_df[\"PredictedLabel\"]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = comparison_df[\"Match\"].mean()\n",
    "\n",
    "# Display results\n",
    "print(\"Comparison of Predicted vs. Actual:\")\n",
    "print(comparison_df[[\"ReviewText\", \"FINAL Fit\", \"PredictedLabel\", \"Match\"]])\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Distribution:\n",
      " FINAL Fit\n",
      "No Comment                 317\n",
      "Correct Size/Just Right    134\n",
      "Wrong Size                  49\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"fit.csv\").head(500)\n",
    "\n",
    "# Check category distribution\n",
    "category_counts = df[\"FINAL Fit\"].value_counts()\n",
    "print(\"Category Distribution:\\n\", category_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few shot First 500 without No Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 55\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Tokenize the input\u001b[39;00m\n\u001b[1;32m     47\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m     48\u001b[0m     prompt,\n\u001b[1;32m     49\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m\n\u001b[1;32m     53\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Send input tensors to GPU\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[1;32m     57\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,  \u001b[38;5;66;03m# Adjust for a longer response window\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,    \u001b[38;5;66;03m# Adds randomness; lower values make output deterministic\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,          \u001b[38;5;66;03m# Nucleus sampling\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     eos_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[1;32m     61\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Decode the response and clean it\u001b[39;00m\n\u001b[1;32m     64\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/transformers/generation/utils.py:2252\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2245\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2246\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2247\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2249\u001b[0m     )\n\u001b[1;32m   2251\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2252\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[1;32m   2253\u001b[0m         input_ids,\n\u001b[1;32m   2254\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   2255\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   2256\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   2257\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   2258\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   2259\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2260\u001b[0m     )\n\u001b[1;32m   2262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2265\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2266\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2272\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/transformers/generation/utils.py:3254\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3252\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3254\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3257\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3258\u001b[0m     outputs,\n\u001b[1;32m   3259\u001b[0m     model_kwargs,\n\u001b[1;32m   3260\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3261\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1163\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1160\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1163\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1164\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1165\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1166\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1167\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1168\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1169\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1170\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1171\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1172\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1173\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1174\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1175\u001b[0m )\n\u001b[1;32m   1177\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:913\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    902\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    903\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         position_embeddings,\n\u001b[1;32m    911\u001b[0m     )\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 913\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    914\u001b[0m         hidden_states,\n\u001b[1;32m    915\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m    916\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    917\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    918\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    919\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    920\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    921\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    922\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    925\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:640\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    641\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    642\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    643\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    644\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    645\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    646\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    647\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    648\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    650\u001b[0m )\n\u001b[1;32m    651\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:523\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m bsz, q_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    522\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[0;32m--> 523\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m    524\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n\u001b[1;32m    526\u001b[0m \u001b[38;5;66;03m# use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/cent7/2024.02-py311/llama2/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:253\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    251\u001b[0m inp_dtype \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    255\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    256\u001b[0m out \u001b[38;5;241m=\u001b[39m bnb\u001b[38;5;241m.\u001b[39mmatmul_4bit(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mt(), bias\u001b[38;5;241m=\u001b[39mbias, quant_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mquant_state)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the file path for the input reviews\n",
    "input_file = \"fit.csv\"\n",
    "output_file = \"fit_predictions_first_500_no_comment.csv\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load input CSV into a pandas DataFrame\n",
    "df_input = pd.read_csv(input_file)\n",
    "\n",
    "# Drop rows where 'FINAL Fit' is 'No Comment'\n",
    "df_input = df_input[df_input[\"FINAL Fit\"] != \"No Comment\"]\n",
    "\n",
    "# Limit to the first 500 rows after filtering\n",
    "df_input = df_input.head(500)\n",
    "\n",
    "# Prepare to write results to a new CSV file\n",
    "with open(output_file, mode=\"w\", newline=\"\") as out_csv:\n",
    "    writer = csv.writer(out_csv)\n",
    "    writer.writerow([\"ReviewText\", \"PredictedLabel\"])  # Write headers\n",
    "\n",
    "    # Process each filtered review\n",
    "    for _, row in df_input.iterrows():\n",
    "        review = row[\"ReviewText\"]\n",
    "\n",
    "        prompt = f\"\"\"### Instruction:\n",
    "Classify the following review into one of the categories: \"Correct Size/Just Right,\" or \"Wrong Size\"\n",
    "Respond only with the category name.\n",
    "\n",
    "### Categories:\n",
    "1. Correct Size/Just Right: The product fits as expected and performs its intended function without issues.\n",
    "2. Wrong Size: The product does not fit or requires modifications to work correctly.\n",
    "\n",
    "### Examples:\n",
    "1. \"I put this into a 1999 Mazda Miata MX-5.  It fit perfectly.  It came fully charged.  Delivered it saved me $50.  Great deal.  Love it.\" -> Correct Size/Just Right\n",
    "2. \"It was not the exact match. I had to rewire the battery in order to make it work. It was a toy for my Lil man. I am glad that I was able to to make it work.  But make sure you can iuse it.\" -> Wrong Size\n",
    "\n",
    "### Review:\n",
    "{review}\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "        # Tokenize the input\n",
    "        inputs = tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=1024\n",
    "        ).to(\"cuda\")  # Send input tensors to GPU\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=20,  # Adjust for a longer response window\n",
    "            temperature=0.7,    # Adds randomness; lower values make output deterministic\n",
    "            top_p=0.9,          # Nucleus sampling\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode the response and clean it\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        # # Extract only the classification label cleanly\n",
    "        # if \"Classification ->\" in response:\n",
    "        #     response = response.split(\"Classification ->\")[-1].split(\"\\n\")[0].strip()\n",
    "        # else:\n",
    "        #     # If no proper format, default to \"No Comment\" for robustness\n",
    "        #     response = \"No Comment\"\n",
    "\n",
    "        # # Write the review and predicted label to the output CSV\n",
    "        # writer.writerow([review, response])\n",
    "\n",
    "print(f\"Predictions for the filtered reviews saved to {output_file}.\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Print runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Runtime: {runtime:.2f} seconds\")\n",
    "\n",
    "# # File paths\n",
    "# predictions_file = \"fit_predictions_first_500.csv\"  # File with model predictions\n",
    "\n",
    "# # Load prediction file as DataFrame\n",
    "# df_predictions = pd.read_csv(predictions_file)\n",
    "\n",
    "# # Combine DataFrames for comparison\n",
    "# # Use \"ReviewText\" as the matching key\n",
    "# comparison_df = pd.merge(\n",
    "#     df_input, \n",
    "#     df_predictions, \n",
    "#     on=\"ReviewText\", \n",
    "#     how=\"inner\"\n",
    "# )\n",
    "\n",
    "# # Compare the 'FINAL Fit' column with 'PredictedLabel'\n",
    "# comparison_df[\"Match\"] = comparison_df[\"FINAL Fit\"] == comparison_df[\"PredictedLabel\"]\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = comparison_df[\"Match\"].mean()\n",
    "\n",
    "# # Display results\n",
    "# print(\"Comparison of Predicted vs. Actual:\")\n",
    "# print(comparison_df[[\"ReviewText\", \"FINAL Fit\", \"PredictedLabel\", \"Match\"]])\n",
    "\n",
    "# print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "You are an assistant tasked with classifying reviews into one of the categories: \"Correct Size/Just Right\" or \"Wrong Size\".\n",
      "Respond **only** with the category name: \"Correct Size/Just Right\" or \"Wrong Size\". Do not include any explanations, links, or additional text.\n",
      "\n",
      "### Categories:\n",
      "1. Correct Size/Just Right: The product fits as expected and performs its intended function without issues.\n",
      "2. Wrong Size: The product does not fit or requires modifications to work correctly.\n",
      "\n",
      "### Examples:\n",
      "1. \"I put this into a 1999 Mazda Miata MX-5. It fit perfectly. It came fully charged. Delivered it saved me $50. Great deal. Love it.\" -> Correct Size/Just Right\n",
      "2. \"It was not the exact match. I had to rewire the battery in order to make it work. It was a toy for my Lil man. I am glad that I was able to make it work. But make sure you can use it.\" -> Wrong Size\n",
      "\n",
      "### Review:\n",
      "Works with the TYC 2354 Honda radiator. Fit on nice and tight with no leaks what so ever. ACDelco is a trusted brand.\n",
      "### Response:\n",
      "Correct Size/Just Right\n",
      "### Review:\n",
      "I got this for my 1997 Toyota Corolla\n",
      "Runtime: 2048.88 seconds\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "You are an assistant tasked with classifying reviews into one of the categories: \"Correct Size/Just Right\" or \"Wrong Size\".\n",
    "Respond **only** with the category name: \"Correct Size/Just Right\" or \"Wrong Size\". Do not include any explanations, links, or additional text.\n",
    "\n",
    "### Categories:\n",
    "1. Correct Size/Just Right: The product fits as expected and performs its intended function without issues.\n",
    "2. Wrong Size: The product does not fit or requires modifications to work correctly.\n",
    "\n",
    "### Examples:\n",
    "1. \"I put this into a 1999 Mazda Miata MX-5. It fit perfectly. It came fully charged. Delivered it saved me $50. Great deal. Love it.\" -> Correct Size/Just Right\n",
    "2. \"It was not the exact match. I had to rewire the battery in order to make it work. It was a toy for my Lil man. I am glad that I was able to make it work. But make sure you can use it.\" -> Wrong Size\n",
    "\n",
    "### Review:\n",
    "Works with the TYC 2354 Honda radiator. Fit on nice and tight with no leaks what so ever. ACDelco is a trusted brand.\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=1024\n",
    ").to(\"cuda\")  # Send input tensors to GPU\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=20,  # Adjust for a longer response window\n",
    "    temperature=0.7,    # Adds randomness; lower values make output deterministic\n",
    "    top_p=0.9,          # Nucleus sampling\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode the response and clean it\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "print(response)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Print runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Runtime: {runtime:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few shot with multiple examples per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for the first 500 reviews saved to fit_predictions_first_500.csv.\n",
      "Runtime: 657.14 seconds\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Define the file path for the input reviews\n",
    "input_file = \"fit.csv\"\n",
    "output_file = \"fit_predictions_first_500.csv\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Prepare to write results to a new CSV file\n",
    "with open(output_file, mode=\"w\", newline=\"\") as out_csv:\n",
    "    writer = csv.writer(out_csv)\n",
    "    writer.writerow([\"ReviewText\", \"PredictedLabel\"])  # Write headers\n",
    "\n",
    "    # Initialize a counter\n",
    "    review_count = 0\n",
    "\n",
    "    # Read and process each review from the input CSV file\n",
    "    with open(input_file, mode=\"r\") as in_csv:\n",
    "        reader = csv.DictReader(in_csv)\n",
    "        for row in reader:\n",
    "            if review_count >= 500:  # Process only the first 10 reviews\n",
    "                break\n",
    "\n",
    "            review = row[\"ReviewText\"]\n",
    "\n",
    "            prompt = f\"\"\"### Instruction:\n",
    "    Classify the following review into one of the categories: \"Correct Size/Just Right,\" \"Wrong Size,\" or \"No Comment.\"\n",
    "    Respond only with the category name.\n",
    "\n",
    "    ### Categories:\n",
    "    1. Correct Size/Just Right: The product fits as expected and performs its intended function without issues.\n",
    "    2. Wrong Size: The product does not fit or requires modifications to work correctly.\n",
    "    3. No Comment: The review does not mention size or fitting issues.\n",
    "\n",
    "    ### Examples:\n",
    "    1. \"I put this into a 1999 Mazda Miata MX-5.  It fit perfectly.  It came fully charged.  Delivered it saved me $50.  Great deal.  Love it.\" -> Correct Size/Just Right\n",
    "    2. \"I have even used this to start my dodge 2500 which has a heavy duty battery for starting and it worked great. The light pulls out to expose a 12v car adapter socket.\" -> Correct Size/Just Right\n",
    "    3. \"It was not the exact match. I had to rewire the battery in order to make it work. It was a toy for my Lil man. I am glad that I was able to to make it work.  But make sure you can iuse it.\" -> Wrong Size\n",
    "    4. \"two different ends on cables. doesn't make sense. had to change the end on one side to fit it to the battery.\" -> Wrong Size\n",
    "    5. \"I would recommend this product.  It lasts long and works fine.  Did the job for me.  It was a good price.\" -> No Comment\n",
    "\n",
    "    ### Review:\n",
    "    {review}\n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "\n",
    "            # Tokenize the input\n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=1500\n",
    "            ).to(\"cuda\")  # Send input tensors to GPU\n",
    "\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=20,  # Adjust for a longer response window\n",
    "                temperature=0.7,    # Adds randomness; lower values make output deterministic\n",
    "                top_p=0.9,          # Nucleus sampling\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            # Decode the response and clean it\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "            # Extract only the classification label cleanly\n",
    "            if \"Classification ->\" in response:\n",
    "                response = response.split(\"Classification ->\")[-1].split(\"\\n\")[0].strip()\n",
    "            else:\n",
    "                # If no proper format, default to \"No Comment\" for robustness\n",
    "                response = \"No Comment\"\n",
    "\n",
    "            # Write the review and predicted label to the output CSV\n",
    "            writer.writerow([review, response])\n",
    "\n",
    "            # Increment the counter\n",
    "            review_count += 1\n",
    "\n",
    "print(f\"Predictions for the first 500 reviews saved to {output_file}.\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Print runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Runtime: {runtime:.2f} seconds\")\n",
    "\n",
    "# File paths\n",
    "input_file = \"fit.csv\"  # Original file with actual labels\n",
    "predictions_file = \"fit_predictions_first_500.csv\"  # File with model predictions\n",
    "\n",
    "# Load input and prediction files as DataFrames\n",
    "df_input = pd.read_csv(input_file)\n",
    "df_predictions = pd.read_csv(predictions_file)\n",
    "\n",
    "df_input = df_input.head(500)\n",
    "\n",
    "# Combine DataFrames for comparison\n",
    "# Use \"ReviewText\" as the matching key\n",
    "comparison_df = pd.merge(\n",
    "    df_input, \n",
    "    df_predictions, \n",
    "    on=\"ReviewText\", \n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Compare the 'FINAL Fit' column with 'PredictedLabel'\n",
    "comparison_df[\"Match\"] = comparison_df[\"FINAL Fit\"] == comparison_df[\"PredictedLabel\"]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = comparison_df[\"Match\"].mean()\n",
    "\n",
    "# Display results\n",
    "print(\"Comparison of Predicted vs. Actual:\")\n",
    "print(comparison_df[[\"ReviewText\", \"FINAL Fit\", \"PredictedLabel\", \"Match\"]])\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Distribution:\n",
      " PredictedLabel\n",
      "No Comment    500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "category_counts = df_predictions[\"PredictedLabel\"].value_counts()\n",
    "print(\"Category Distribution:\\n\", category_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path for the input reviews\n",
    "input_file = \"fit.csv\"\n",
    "output_file = \"fit_predictions_full_dataset.csv\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Prepare to write results to a new CSV file\n",
    "with open(output_file, mode=\"w\", newline=\"\") as out_csv:\n",
    "    writer = csv.writer(out_csv)\n",
    "    writer.writerow([\"ReviewText\", \"PredictedLabel\"])  # Write headers\n",
    "\n",
    "    # Read and process each review from the input CSV file\n",
    "    with open(input_file, mode=\"r\") as in_csv:\n",
    "        reader = csv.DictReader(in_csv)\n",
    "        for row in reader:\n",
    "            review = row[\"ReviewText\"]\n",
    "\n",
    "            prompt = f\"\"\"### Instruction:\n",
    "    Classify the following review into one of the categories: \"Correct Size/Just Right,\" \"Wrong Size,\" or \"No Comment.\"\n",
    "    Respond only with the category name.\n",
    "\n",
    "    ### Categories:\n",
    "    1. Correct Size/Just Right: The product fits as expected and performs its intended function without issues.\n",
    "    2. Wrong Size: The product does not fit or requires modifications to work correctly.\n",
    "    3. No Comment: The review does not mention size or fitting issues.\n",
    "\n",
    "    ### Examples:\n",
    "    1. \"I put this into a 1999 Mazda Miata MX-5.  It fit perfectly.  It came fully charged.  Delivered it saved me $50.  Great deal.  Love it.\" -> Correct Size/Just Right\n",
    "    2. \"I have even used this to start my dodge 2500 which has a heavy duty battery for starting and it worked great. The light pulls out to expose a 12v car adapter socket.\" -> Correct Size/Just Right\n",
    "    3. \"It was not the exact match. I had to rewire the battery in order to make it work. It was a toy for my Lil man. I am glad that I was able to to make it work.  But make sure you can iuse it.\" -> Wrong Size\n",
    "    4. \"two different ends on cables. doesn't make sense. had to change the end on one side to fit it to the battery.\" -> Wrong Size\n",
    "    5. \"I would recommend this product.  It lasts long and works fine.  Did the job for me.  It was a good price.\" -> No Comment\n",
    "\n",
    "    ### Review:\n",
    "    {review}\n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "\n",
    "            # Tokenize the input\n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=1500\n",
    "            ).to(\"cuda\")  # Send input tensors to GPU\n",
    "\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=20,  # Adjust for a longer response window\n",
    "                temperature=0.7,    # Adds randomness; lower values make output deterministic\n",
    "                top_p=0.9,          # Nucleus sampling\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            # Decode the response and clean it\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "            # Extract only the classification label cleanly\n",
    "            if \"Classification ->\" in response:\n",
    "                response = response.split(\"Classification ->\")[-1].split(\"\\n\")[0].strip()\n",
    "            else:\n",
    "                # If no proper format, default to \"No Comment\" for robustness\n",
    "                response = \"No Comment\"\n",
    "\n",
    "            # Write the review and predicted label to the output CSV\n",
    "            writer.writerow([review, response])\n",
    "\n",
    "print(f\"Predictions for the entire dataset saved to {output_file}.\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Print runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Runtime: {runtime:.2f} seconds\")\n",
    "\n",
    "# File paths\n",
    "input_file = \"fit.csv\"  # Original file with actual labels\n",
    "predictions_file = \"fit_predictions_full_dataset.csv\"  # File with model predictions\n",
    "\n",
    "# Load input and prediction files as DataFrames\n",
    "df_input = pd.read_csv(input_file)\n",
    "df_predictions = pd.read_csv(predictions_file)\n",
    "\n",
    "# Combine DataFrames for comparison\n",
    "# Use \"ReviewText\" as the matching key\n",
    "comparison_df = pd.merge(\n",
    "    df_input, \n",
    "    df_predictions, \n",
    "    on=\"ReviewText\", \n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Compare the 'FINAL Fit' column with 'PredictedLabel'\n",
    "comparison_df[\"Match\"] = comparison_df[\"FINAL Fit\"] == comparison_df[\"PredictedLabel\"]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = comparison_df[\"Match\"].mean()\n",
    "\n",
    "# Display results\n",
    "print(\"Comparison of Predicted vs. Actual:\")\n",
    "print(comparison_df[[\"ReviewText\", \"FINAL Fit\", \"PredictedLabel\", \"Match\"]])\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
