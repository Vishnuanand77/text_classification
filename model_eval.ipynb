{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(actual_df, prediction_df):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of predicted labels against actual labels.\n",
    "\n",
    "    Args:\n",
    "        actual_df (pd.DataFrame): DataFrame containing the actual labels with \"ReviewText\" and \"label\".\n",
    "        prediction_df (pd.DataFrame): DataFrame containing the predicted labels with \"ReviewText\" and \"PredictedLabel\".\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing accuracy, precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    # Combine the actual and predicted DataFrames on \"ReviewText\"\n",
    "    comparison_df = pd.merge(\n",
    "        actual_df, \n",
    "        prediction_df, \n",
    "        on=\"ReviewText\", \n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    if comparison_df.empty:\n",
    "        raise ValueError(\"No matching ReviewText found between actual and predicted data.\")\n",
    "\n",
    "    # Check for matches between 'label' and 'PredictedLabel'\n",
    "    comparison_df[\"Match\"] = comparison_df[\"label\"] == comparison_df[\"PredictedLabel\"]\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = comparison_df[\"Match\"].mean()\n",
    "\n",
    "    # Convert labels to numeric for precision, recall, and F1 score\n",
    "    y_true = comparison_df[\"label\"]\n",
    "    y_pred = comparison_df[\"PredictedLabel\"]\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    precision = precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "    # Print classification report for more insights (optional)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    # Return the evaluation metrics\n",
    "    return {\n",
    "        \"accuracy\": accuracy * 100,\n",
    "        \"precision\": precision * 100,\n",
    "        \"recall\": recall * 100,\n",
    "        \"f1_score\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: ['Satisfied With Vendor' 'Non-satisfaction With' 'No Comment']\n",
      "Counts of each category:\n",
      "Satisfied With Vendor    1937\n",
      "Non-satisfaction With     313\n",
      "No Comment                  5\n",
      "Name: PredictedLabel, dtype: int64\n",
      "\n",
      "Classification Report:\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "                  No Comment       1.00      0.00      0.00      2068\n",
      "Non-satisfaction With Vendor       0.16      0.62      0.25        79\n",
      "       Satisfied With Vendor       0.05      0.98      0.10       108\n",
      "\n",
      "                    accuracy                           0.07      2255\n",
      "                   macro avg       0.40      0.53      0.12      2255\n",
      "                weighted avg       0.93      0.07      0.02      2255\n",
      "\n",
      "Accuracy: 7.10%\n",
      "Precision: 92.52%\n",
      "Recall: 7.10%\n",
      "F1 Score: 0.02\n"
     ]
    }
   ],
   "source": [
    "input_file = \"vendor.csv\"  # Original file with actual labels\n",
    "predictions_file = \"results/vendor_predictions_one_shot_full_dataset.csv\"  # File with model predictions\n",
    "\n",
    "# Load input and prediction files as DataFrames\n",
    "df_input = pd.read_csv(input_file)\n",
    "df_predictions = pd.read_csv(predictions_file)\n",
    "\n",
    "# Check unique values in the 'label' column\n",
    "unique_labels = df_predictions['PredictedLabel'].unique()\n",
    "print(f\"Unique labels: {unique_labels}\")\n",
    "\n",
    "# Count occurrences of each label\n",
    "label_counts = df_predictions['PredictedLabel'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(\"Counts of each category:\")\n",
    "print(label_counts)\n",
    "\n",
    "label_mapping = {\n",
    "    'Non-satisfaction With': 'Non-satisfaction With Vendor'\n",
    "}\n",
    "\n",
    "# Replace incorrect labels with standardized ones\n",
    "df_predictions['PredictedLabel'] = df_predictions['PredictedLabel'].replace(label_mapping)\n",
    "\n",
    "# Evaluate predictions\n",
    "metrics = evaluate_predictions(df_input, df_predictions)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {metrics['accuracy']:.2f}%\")\n",
    "print(f\"Precision: {metrics['precision']:.2f}%\")\n",
    "print(f\"Recall: {metrics['recall']:.2f}%\")\n",
    "print(f\"F1 Score: {metrics['f1_score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
